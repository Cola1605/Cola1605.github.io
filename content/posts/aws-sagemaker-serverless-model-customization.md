---
title: "Amazon SageMaker AI Serverless Customization - TÄƒng Tá»‘c Model Fine-Tuning"
date: 2025-12-08
draft: false
description: "SageMaker AI ra máº¯t Serverless Customization - rÃºt ngáº¯n model customization tá»« thÃ¡ng xuá»‘ng ngÃ y, há»— trá»£ DPO, RLVR, RLAIF vá»›i UI Ä‘Æ¡n giáº£n vÃ  MLflow tÃ­ch há»£p"
tags: ["AWS", "SageMaker AI", "Serverless", "Fine-Tuning", "Model Customization", "DPO", "RLVR", "RLAIF", "MLflow", "Amazon Nova", "AWS re:Invent"]
categories: ["AI and Machine Learning", "DevOps and Infrastructure"]
author: "Channy Yun"
language: "vi"
slug: "aws-sagemaker-serverless-model-customization"
---

# Amazon SageMaker AI Serverless Customization - TÄƒng Tá»‘c Model Fine-Tuning

## ğŸ“‹ TÃ³m Táº¯t

NgÃ y 3/12/2025, AWS cÃ´ng bá»‘ **Serverless Customization** cho Amazon SageMaker AI - tÃ­nh nÄƒng Ä‘á»™t phÃ¡ giÃºp:

- âš¡ **RÃºt ngáº¯n customization tá»« thÃ¡ng â†’ ngÃ y**
- ğŸ¯ **Chá»‰ vÃ i click** Ä‘á»ƒ fine-tune model
- ğŸ”§ Há»— trá»£ **DPO, RLVR, RLAIF** vÃ  supervised fine-tuning
- ğŸš€ **HoÃ n toÃ n serverless** - khÃ´ng lo infrastructure
- ğŸ“Š **MLflow tÃ­ch há»£p** - tá»± Ä‘á»™ng track experiments
- ğŸ’° **Pay-per-token** - chá»‰ tráº£ tiá»n cho tokens Ä‘Ã£ xá»­ lÃ½

**Models há»— trá»£**: Amazon Nova, DeepSeek, Qwen, Meta Llama

---

## ğŸ¯ ThÃ´ng Tin BÃ i Viáº¿t

| ThÃ´ng Tin | Chi Tiáº¿t |
|-----------|----------|
| **TÃ¡c giáº£** | Channy Yun (ìœ¤ì„ì°¬) |
| **NgÃ y xuáº¥t báº£n** | 8/12/2025 |
| **NgÃ y cÃ´ng bá»‘** | 3/12/2025 (AWS re:Invent) |
| **Regions** | US East, US West, Tokyo, Ireland |

---

## ğŸŒŸ Äá»™t PhÃ¡ ChÃ­nh

### Tá»« ThÃ¡ng â†’ NgÃ y

**TrÆ°á»›c Ä‘Ã¢y**:
- ğŸ”´ Setup infrastructure phá»©c táº¡p
- ğŸ”´ Viáº¿t code training tá»« Ä‘áº§u
- ğŸ”´ Manage compute resources
- ğŸ”´ Máº¥t vÃ i thÃ¡ng

**BÃ¢y giá»**:
- âœ… Chá»‰ vÃ i click trÃªn UI
- âœ… Auto-provision resources
- âœ… HoÃ n toÃ n serverless
- âœ… **HoÃ n thÃ nh trong vÃ i ngÃ y**

---

## ğŸ› ï¸ 4 PhÆ°Æ¡ng PhÃ¡p Customization

### 1. Supervised Fine-Tuning
**Truyá»n thá»‘ng**, phÃ¹ há»£p khi cÃ³ labeled data

### 2. DPO - Direct Preference Optimization
**Tá»‘i Æ°u preference trá»±c tiáº¿p**, khÃ´ng cáº§n reward model

### 3. RLVR - RL with Verifiable Rewards
**Objective tasks** vá»›i rewards cÃ³ thá»ƒ verify (code, math)

### 4. RLAIF - RL from AI Feedback
**Subjective tasks** dÃ¹ng AI judge

**Chá»n method dá»±a trÃªn**:
- Dataset size vÃ  quality
- Customization goals
- Task type (objective vs subjective)

---

## ğŸ¨ 2 CÃ¡ch Sá»­ Dá»¥ng

### Option 1: UI Customization (Dá»… nháº¥t!)

**9 bÆ°á»›c Ä‘Æ¡n giáº£n**:

```
1. SageMaker Studio â†’ Models
   â†“
2. Chá»n model (e.g., Llama 3.1 8B)
   â†“
3. "Customize in UI"
   â†“
4. Chá»n method (DPO/RLVR/RLAIF/Supervised)
   â†“
5. Upload training dataset
   â†“
6. Configure hyperparameters
   â†“
7. Submit â†’ Training starts!
   â†“
8. Evaluate vs base model
   â†“
9. Deploy to Bedrock/SageMaker
```

**Features**:
- âœ… Recommended hyperparameters
- âœ… Serverless MLflow tracking
- âœ… Network & storage encryption
- âœ… Auto resource provisioning

### Option 2: Code Customization (Flexible!)

**Cho advanced users**:

```python
# Sample notebook tá»± Ä‘á»™ng generate
# CÃ³ thá»ƒ edit trong JupyterLab

# Deploy options:
- SageMaker Inference
- SageMaker HyperPod
```

---

## ğŸš€ Workflow Chi Tiáº¿t

### Step 1: Training

```
Select Model â†’ Choose Method â†’ Upload Data â†’ Configure â†’ Train
```

**Auto features**:
- Resource provisioning
- Optimal compute selection
- Progress monitoring

### Step 2: Post-Training Actions

#### Continue Customization
- Adjust hyperparameters
- Try different methods
- Iterative improvement

#### Evaluation
```
Compare:
- Custom model vs Base model
- Different customization methods
- Various hyperparameter settings
```

#### Deployment

**Option A: Amazon Bedrock (Serverless)**
```
Deploy â†’ Bedrock Console â†’ Test inference
```
- âœ… Fully serverless
- âœ… Auto-scaling
- âœ… Pay-per-use

**Option B: SageMaker Endpoint (Control)**
```
Deploy â†’ Configure:
- Instance type
- Instance count
- Endpoint settings
```
- âœ… Resource control
- âœ… Dedicated capacity
- âœ… Predictable performance

### Step 3: Testing

**Playground tab**:
- Single prompt mode
- Chat mode
- Side-by-side comparison

---

## ğŸ“Š MLflow Integration

### Serverless MLflow

**Tá»± Ä‘á»™ng track**:
- Training metrics
- Hyperparameters
- Model versions
- Experiment comparisons

**Visualizations**:
- Loss curves
- Accuracy trends
- Hyperparameter impact
- Model comparison charts

**Zero code changes** required!

---

## ğŸ’° Pricing Model

### Pay-per-Token

| Component | Pricing |
|-----------|---------|
| **Training** | Tokens processed during training |
| **Inference** | Tokens processed during inference |
| **Storage** | Model storage (standard S3 rates) |

**No charges for**:
- Infrastructure management
- Idle time
- Failed experiments

**Details**: [SageMaker AI Pricing](https://aws.amazon.com/sagemaker/ai/pricing/)

---

## ğŸ¯ Use Cases

### 1. Domain-Specific Models

**Scenario**: Legal, Medical, Financial domain

**Solution**:
```
Base Model (Llama 3.1)
    â†“
+ Domain data
    â†“
Supervised Fine-Tuning
    â†“
Domain Expert Model
```

### 2. Instruction Following

**Scenario**: Chatbot cáº§n follow specific guidelines

**Solution**:
```
Base Model
    â†“
+ Preference data
    â†“
DPO/RLAIF
    â†“
Guideline-Compliant Model
```

### 3. Code Generation

**Scenario**: Internal coding standards

**Solution**:
```
Base Model
    â†“
+ Code examples + Test cases
    â†“
RLVR (verifiable rewards)
    â†“
Standards-Compliant Code Generator
```

---

## âœ… Best Practices

### Dataset Preparation

**DO**:
- âœ… Clean, high-quality data
- âœ… Representative samples
- âœ… Proper format (method-specific)
- âœ… Sufficient volume

**DON'T**:
- âŒ Noisy/duplicate data
- âŒ Biased samples
- âŒ Wrong format
- âŒ Too small dataset

### Method Selection

| Task Type | Recommended Method |
|-----------|-------------------|
| **Labeled data available** | Supervised Fine-Tuning |
| **Preference pairs** | DPO |
| **Objective evaluation** | RLVR |
| **Subjective quality** | RLAIF |

### Hyperparameter Tuning

**Start with defaults** â†’ Monitor metrics â†’ Adjust if needed

**Key parameters**:
- Learning rate
- Batch size
- Epochs
- Warmup steps

---

## ğŸŒ Availability

### Regions

- âœ… US East (Virginia)
- âœ… US West (Oregon)
- âœ… Asia Pacific (Tokyo)
- âœ… Europe (Ireland)

### Model Support

**Launch time**:
- Amazon Nova
- DeepSeek
- Qwen  
- Meta Llama 3.1 8B Instruct

**More models coming soon!**

---

## ğŸ“ˆ Benefits Summary

| Aspect | Improvement |
|--------|-------------|
| **Time to deploy** | Months â†’ Days |
| **Infrastructure** | Complex â†’ Serverless |
| **Expertise needed** | Deep ML â†’ Basic understanding |
| **Cost model** | Fixed â†’ Pay-per-token |
| **Experiment tracking** | Manual â†’ Auto (MLflow) |
| **Deployment** | Complex â†’ Few clicks |

---

## ğŸ”— Resources

### Documentation
- [SageMaker Developer Guide](https://docs.aws.amazon.com/sagemaker/latest/dg/whatis.html)
- [Model Customization Docs](https://aws.amazon.com/sagemaker/ai/model-customization/)

### Consoles
- [SageMaker Studio](https://console.aws.amazon.com/sagemaker)
- [Bedrock Console](https://console.aws.amazon.com/bedrock)

### Support
- [AWS re:Post](https://repost.aws/tags/TAT80swPyVRPKPcA0rsJYPuA/amazon-sagemaker)
- AWS Support

---

## ğŸ’¡ Káº¿t Luáº­n

Amazon SageMaker AI Serverless Customization **democratizes model fine-tuning**:

âœ… **RÃºt ngáº¯n thá»i gian** - Tá»« thÃ¡ng xuá»‘ng ngÃ y  
âœ… **ÄÆ¡n giáº£n hÃ³a** - Chá»‰ vÃ i click  
âœ… **Serverless** - KhÃ´ng lo infrastructure  
âœ… **Advanced methods** - DPO, RLVR, RLAIF  
âœ… **Auto tracking** - MLflow integrated  
âœ… **Flexible deployment** - Bedrock hoáº·c SageMaker  
âœ… **Cost-effective** - Pay-per-token  

**Báº¯t Ä‘áº§u ngay**: [SageMaker Studio Console](https://console.aws.amazon.com/sagemaker)

---

**ChÃºc báº¡n fine-tuning thÃ nh cÃ´ng!**  
â€” **Channy Yun**

---

## ğŸ”— BÃ i Viáº¿t LiÃªn Quan

1. [SageMaker HyperPod Checkpointless Training](https://cola1605.github.io/posts/aws-sagemaker-hyperpod-checkpointless-elastic-training/)
2. [Bedrock Reinforcement Fine-Tuning](https://cola1605.github.io/posts/amazon-bedrock-reinforcement-fine-tuning/)
3. [Amazon Nova Models](https://aws.amazon.com/nova/)
