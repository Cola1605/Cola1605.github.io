---
title: "Thiáº¿t Káº¿ MÃ´i TrÆ°á»ng Development KhÃ´ng Cáº§n K8s trong Bucketeer"
date: 2025-12-06T10:00:00+09:00
draft: false
categories: ["DevOps and Infrastructure", "Development"]
tags: ["Docker Compose", "Kubernetes", "Redis Streams", "Nginx", "gRPC", "Feature Flags", "Golang", "Microservices", "Development Environment", "Open Source"]
author: "é»’ç”° (Kuroda)"
translator: "æ—¥å¹³"
description: "HÆ°á»›ng dáº«n chi tiáº¿t vá» cÃ¡ch Bucketeer - ná»n táº£ng feature flag management vÃ  A/B testing OSS cá»§a CyberAgent - thiáº¿t káº¿ mÃ´i trÆ°á»ng development dá»±a trÃªn Docker Compose thay tháº¿ Minikube/Kubernetes. BÃ i viáº¿t giáº£i thÃ­ch giáº£i phÃ¡p sá»­ dá»¥ng Redis Streams cho event-driven messaging vÃ  Nginx routing cho gRPC/gRPC-Web/REST."
---

# Thiáº¿t Káº¿ MÃ´i TrÆ°á»ng Development KhÃ´ng Cáº§n K8s trong Bucketeer

**TÃ¡c giáº£**: é»’ç”° ([@knkurokuro7](https://x.com/knkurokuro7))  
**NgÃ y xuáº¥t báº£n**: 6 thÃ¡ng 12 nÄƒm 2025  
**Danh má»¥c**: Engineer  
**Nguá»“n**: [CyberAgent Developers Blog](https://developers.cyberagent.co.jp/blog/archives/60317/)  
**Advent Calendar**: CyberAgent Developers Advent Calendar 2025 Day 6

---

## Giá»›i Thiá»‡u

ÄÃ¢y lÃ  bÃ i viáº¿t ngÃ y thá»© 6 cá»§a [CyberAgent Developers Advent Calendar 2025](https://adventar.org/calendars/11590)! ğŸ¤¶

Xin chÃ o! TÃ´i lÃ  Kuroda ([@knkurokuro7](https://x.com/knkurokuro7)) tá»« CA Dev Platform Bucketeer Team thuá»™c Technical Policy Division.

[Bucketeer](https://github.com/bucketeer-io/bucketeer) lÃ  ná»n táº£ng OSS feature flag management vÃ  A/B testing Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi CyberAgent. Äá»ƒ tÃ¬m hiá»ƒu vá» khá»Ÿi Ä‘áº§u cá»§a viá»‡c OSS hÃ³a Bucketeer, vui lÃ²ng tham kháº£o [bÃ i viáº¿t nÃ y](https://www.cyberagent.co.jp/way/list/detail/id=28416).

TrÆ°á»›c Ä‘Ã¢y, mÃ´i trÆ°á»ng development local cá»§a Bucketeer Ä‘Æ°á»£c xÃ¢y dá»±ng dá»±a trÃªn Minikube (Kubernetes), nhÆ°ng gáº§n Ä‘Ã¢y chÃºng tÃ´i Ä‘Ã£ thÃªm mÃ´i trÆ°á»ng má»›i sá»­ dá»¥ng Docker Compose.

MÃ´i trÆ°á»ng Minikube cho phÃ©p xÃ¡c nháº­n hoáº¡t Ä‘á»™ng gáº§n vá»›i production, nhÆ°ng Ä‘á»•i láº¡i yÃªu cáº§u kiáº¿n thá»©c vá» Kubernetes vÃ  tiÃªu tá»‘n nhiá»u tÃ i nguyÃªn do khá»Ÿi Ä‘á»™ng VM, khiáº¿n nÃ³ hÆ¡i náº·ng ná» cho viá»‡c xÃ¡c nháº­n Ä‘Æ¡n giáº£n. HÆ¡n ná»¯a, cÅ©ng cÃ³ Ã½ kiáº¿n ráº±ng "sáº½ tiá»‡n lá»£i hÆ¡n náº¿u cÃ³ thá»ƒ khá»Ÿi Ä‘á»™ng má»i thá»© chá»‰ vá»›i má»™t lá»‡nh `docker compose up`", nÃªn chÃºng tÃ´i quyáº¿t Ä‘á»‹nh thá»±c hiá»‡n tÃ­ch há»£p Docker Compose.

Tuy nhiÃªn, Ä‘Ã¢y khÃ´ng pháº£i lÃ  cÃ´ng viá»‡c Ä‘Æ¡n giáº£n chá»‰ cáº§n sáº¯p xáº¿p cÃ¡c Docker containers vÃ  nÃ³i "xong!".

Bucketeer Ã¡p dá»¥ng kiáº¿n trÃºc event-driven sá»­ dá»¥ng Google Cloud Pub/Sub, vÃ  thÃ¡ch thá»©c lÃ  lÃ m tháº¿ nÃ o Ä‘á»ƒ tÃ¡i táº¡o Ä‘iá»u nÃ y trong mÃ´i trÆ°á»ng Docker Compose. NgoÃ i ra, cÅ©ng cáº§n xá»­ lÃ½ 3 phÆ°Æ¡ng thá»©c giao tiáº¿p (gRPC, gRPC-Web, REST) thÃ´ng qua má»™t entry point duy nháº¥t.

BÃ i viáº¿t nÃ y sáº½ giá»›i thiá»‡u cÃ¡ch chÃºng tÃ´i giáº£i quyáº¿t hai thÃ¡ch thá»©c nÃ y.

---

## So SÃ¡nh Kiáº¿n TrÃºc

Äáº§u tiÃªn, hÃ£y so sÃ¡nh kiáº¿n trÃºc cá»§a cáº£ hai mÃ´i trÆ°á»ng báº±ng sÆ¡ Ä‘á»“.

### 1. MÃ´i TrÆ°á»ng Minikube (Kubernetes)

**Messaging**: Google Cloud Pub/Sub Emulator  
**Service Discovery**: Kubernetes Service Discovery + Envoy

**Äáº·c Ä‘iá»ƒm**:
- CÃ³ thá»ƒ xÃ¡c nháº­n hoáº¡t Ä‘á»™ng gáº§n vá»›i production
- YÃªu cáº§u kiáº¿n thá»©c vá» Kubernetes
- TiÃªu tá»‘n nhiá»u tÃ i nguyÃªn do khá»Ÿi Ä‘á»™ng VM
- HÆ¡i náº·ng ná» cho viá»‡c xÃ¡c nháº­n Ä‘Æ¡n giáº£n

### 2. MÃ´i TrÆ°á»ng Docker Compose

**Messaging**: Redis Streams  
**Service Discovery**: Nginx Reverse Proxy

**Äáº·c Ä‘iá»ƒm**:
- Khá»Ÿi Ä‘á»™ng má»i thá»© chá»‰ vá»›i má»™t lá»‡nh `docker compose up`
- Hiá»‡u quáº£ vá» tÃ i nguyÃªn
- Táº­n dá»¥ng háº¡ táº§ng Redis hiá»‡n cÃ³
- Chi phÃ­ triá»ƒn khai tháº¥p

### Äiá»ƒm KhÃ¡c Biá»‡t ChÃ­nh

CÃ¡c Ä‘iá»ƒm khÃ¡c biá»‡t chÃ­nh lÃ  **pháº§n messaging** vÃ  **pháº§n service discovery**. HÃ£y cÃ¹ng tÃ¬m hiá»ƒu chi tiáº¿t tá»«ng pháº§n.

---

## 1. Messaging vá»›i Redis Streams

Bucketeer Ã¡p dá»¥ng kiáº¿n trÃºc event-driven sá»­ dá»¥ng Google Cloud Pub/Sub.

Trong mÃ´i trÆ°á»ng Minikube, chÃºng tÃ´i sá»­ dá»¥ng Google Cloud Pub/Sub Emulator (hiá»‡n táº¡i Ä‘Ã£ chuyá»ƒn sang Redis Streams). Máº·c dÃ¹ tiá»‡n lá»£i khi cÃ³ thá»ƒ develop vá»›i interface giá»‘ng production, nhÆ°ng cho mÃ´i trÆ°á»ng Docker Compose, chÃºng tÃ´i Ä‘Ã£ Ã¡p dá»¥ng Redis Streams.

Google Cloud Pub/Sub Emulator cÅ©ng cÃ³ thá»ƒ sá»­ dá»¥ng trong mÃ´i trÆ°á»ng Docker Compose, nhÆ°ng cáº§n thÃªm containers. NgoÃ i ra, vÃ¬ há»‡ thá»‘ng hiá»‡n táº¡i Ä‘Ã£ sá»­ dá»¥ng Redis lÃ m cache, chÃºng tÃ´i Ä‘Ã£ chá»n Redis Streams do chi phÃ­ triá»ƒn khai tháº¥p vÃ  hiá»‡u quáº£ vá» tÃ i nguyÃªn.

### PhÃ¢n TÃ¡n Messages Báº±ng FNV Hash

Trong implementation cá»§a Redis Streams, chÃºng tÃ´i sá»­ dá»¥ng **16 partitions** vÃ  phÃ¢n tÃ¡n messages Ä‘á»u Ä‘áº·n báº±ng **FNV-1a hash**.

LÃ½ do chá»n 16 lÃ  vÃ¬ náº¿u sá»‘ lÆ°á»£ng partitions quÃ¡ nhiá»u sáº½ phá»©c táº¡p trong quáº£n lÃ½, vÃ  náº¿u quÃ¡ Ã­t thÃ¬ hiá»‡u quáº£ xá»­ lÃ½ song song sáº½ giáº£m, nÃªn Ä‘Ã¢y lÃ  sá»‘ cÃ¢n báº±ng tá»‘t trong thá»±c táº¿.

**FNV-1a (Fowler-Noll-Vo 1a)** lÃ  hÃ m hash khÃ´ng mÃ£ hÃ³a nhanh vÃ  Ä‘Æ¡n giáº£n. NÃ³ chuyá»ƒn Ä‘á»•i chuá»—i hoáº·c byte array cÃ³ Ä‘á»™ dÃ i báº¥t ká»³ thÃ nh giÃ¡ trá»‹ hash cÃ³ Ä‘á»™ dÃ i cá»‘ Ä‘á»‹nh, vÃ  vÃ¬ phÃ¢n tÃ¡n giÃ¡ trá»‹ hash Ä‘á»u Ä‘áº·n nÃªn phÃ¹ há»£p cho viá»‡c phÃ¢n chia partitions.

```go
// pkg/pubsub/redis/stream_publisher.go

// calculatePartition computes the partition index for a given key.
func (p *StreamPublisher) calculatePartition(key string) int {
    hasher := fnv.New32a()
    _, err := hasher.Write([]byte(key))
    if err != nil {
        // Should not normally error.
        p.logger.Error("Error hashing key", zap.Error(err), zap.String("key", key))
        return 0
    }
    return int(hasher.Sum32() % uint32(p.partitionCount))
}
```

Luá»“ng xá»­ lÃ½ nhÆ° sau:

1. Hash message ID báº±ng `fnv.New32a()`
2. TÃ­nh pháº§n dÆ° khi chia giÃ¡ trá»‹ hash cho sá»‘ partitions (16)
3. Pháº§n dÆ° Ä‘Ã³ trá»Ÿ thÃ nh sá»‘ partition

VÃ¬ cÃ¹ng má»™t message ID luÃ´n Ä‘i Ä‘áº¿n cÃ¹ng má»™t partition, nÃªn thá»© tá»± cá»§a cÃ¡c messages cÃ³ cÃ¹ng key Ä‘Æ°á»£c duy trÃ¬ trong partition.

### Äá»‹nh Dáº¡ng Stream Key

Táº¡o stream key cá»§a Redis cho má»—i partition.

```go
// pkg/pubsub/redis/stream_publisher.go

// getStreamKey returns the partitioned stream name
func (p *StreamPublisher) getStreamKey(id string) string {
    partition := p.calculatePartition(id)
    return fmt.Sprintf("%s-%d{stream}", p.streamBase, partition)
}
```

VÃ­ dá»¥, náº¿u topic `domain` vÃ  partition 5, key sáº½ lÃ  `domain-5{stream}`.

Hash tag `{stream}` lÃ  Ä‘á»ƒ há»— trá»£ Redis Cluster. Khi Ä‘á»c nhiá»u streams báº±ng `XReadGroup`, táº¥t cáº£ cÃ¡c keys pháº£i Ä‘Æ°á»£c Ä‘áº·t trÃªn cÃ¹ng má»™t node, do Ä‘Ã³ chÃºng tÃ´i sá»­ dá»¥ng hash tag cá»‘ Ä‘á»‹nh Ä‘á»ƒ Ä‘áº·t táº¥t cáº£ partitions trÃªn cÃ¹ng má»™t node.

### Äá»c Messages Báº±ng Consumer Group

Consumer Group cá»§a Redis Streams lÃ  cÆ¡ cháº¿ phÃ¢n tÃ¡n messages giá»¯a nhiá»u consumers. KhÃ¡i niá»‡m tÆ°Æ¡ng tá»± nhÆ° consumer group cá»§a Kafka, cÃ¡c consumers trong cÃ¹ng má»™t group sáº½ nháº­n cÃ¡c messages khÃ¡c nhau.

Puller táº¡o Consumer Groups cho táº¥t cáº£ partitions khi khá»Ÿi Ä‘á»™ng.

```go
// pkg/pubsub/redis/stream_puller.go

// Pull reads messages from the stream and calls the handler for each message
func (p *StreamPuller) Pull(ctx context.Context, handler func(context.Context, *puller.Message)) error {
    // ...bá» qua...

    // Create consumer groups for all partitions
    for partition := 0; partition < p.partitionCount; partition++ {
        streamKey := p.getStreamKey(partition)

        // Check if the consumer group exists before attempting to create it
        groupExists, err := p.consumerGroupExists(ctx, streamKey, p.subscription)
        // ...bá» qua...

        // Only create the group if it doesn't exist
        if !groupExists {
            err := p.redisClient.XGroupCreateMkStream(streamKey, p.subscription, "0")
            // ...bá» qua...
        }
    }

    // ...bá» qua...
}
```

Viá»‡c Ä‘á»c messages Ä‘Æ°á»£c thá»±c hiá»‡n báº±ng lá»‡nh `XREADGROUP`. Lá»‡nh nÃ y cÃ³ thá»ƒ láº¥y messages tá»« nhiá»u streams trong má»™t láº§n gá»i. Tuy nhiÃªn, trong implementation nÃ y, má»™t Puller Ä‘á»c táº¥t cáº£ partitions vá»›i má»™t Goroutine duy nháº¥t vÃ  messages Ä‘Æ°á»£c xá»­ lÃ½ tuáº§n tá»±.

Äá»ƒ cáº£i thiá»‡n throughput, cáº§n khá»Ÿi Ä‘á»™ng nhiá»u Puller instances (processes) Ä‘á»ƒ load balancing trong Consumer Group.

```go
// pkg/pubsub/redis/stream_puller.go

// Build streams argument for XREADGROUP
streams := make([]string, 0, p.partitionCount*2)
for partition := 0; partition < p.partitionCount; partition++ {
    streamKey := p.getStreamKey(partition)
    streams = append(streams, streamKey)
}

// Add the IDs matching each stream key (must have same number of IDs as keys)
for partition := 0; partition < p.partitionCount; partition++ {
    streams = append(streams, ">") // ">" means only new messages
}

// Read from all partitions
streamResults, err := p.redisClient.XReadGroup(
    ctx,
    p.subscription,
    p.consumer,
    streams,
    p.batchSize,
    p.blockTime,
)
```

Äá»‹nh dáº¡ng arguments cá»§a `XREADGROUP` lÃ  `[key1, key2, ..., keyN, id1, id2, ..., idN]`. VÃ­ dá»¥ nhÆ° `["stream-0{stream}", "stream-1{stream}", ">", ">"]`, Ä‘áº§u tiÃªn sáº¯p xáº¿p cÃ¡c stream keys, sau Ä‘Ã³ sáº¯p xáº¿p cÃ¡c IDs tÆ°Æ¡ng á»©ng. `>` lÃ  ID specification dÃ nh riÃªng cho Consumer Group, láº¥y "messages chÆ°a Ä‘Æ°á»£c gá»­i Ä‘áº¿n group nÃ y".

### Reclaim Stale Messages

Khi worker crashes, messages Ä‘ang Ä‘Æ°á»£c xá»­ lÃ½ sáº½ bá»‹ "treo". ChÃºng ta gá»i Ä‘Ã¢y lÃ  "stale messages" (messages bá»‹ tá»“n Ä‘á»ng).

Trong Redis Streams, cÃ¡c messages Ä‘ang Ä‘Æ°á»£c xá»­ lÃ½ trong Consumer Group Ä‘Æ°á»£c ghi láº¡i trong "pending entries list". Náº¿u xá»­ lÃ½ hoÃ n táº¥t bÃ¬nh thÆ°á»ng, sáº½ gá»­i acknowledgment báº±ng `XACK` vÃ  xÃ³a khá»i list. Tuy nhiÃªn, náº¿u worker crashes, acknowledgment khÃ´ng Ä‘Æ°á»£c gá»­i vÃ  messages sáº½ tiáº¿p tá»¥c tá»“n táº¡i.

Äá»ƒ giáº£i quyáº¿t váº¥n Ä‘á» nÃ y, chÃºng tÃ´i Ä‘Ã£ implement `recoveryLoop` Ä‘á»ƒ thu há»“i stale messages trong background.

```go
// pkg/pubsub/redis/stream_puller.go

// recoveryLoop periodically checks for stale messages and reclaims them
func (p *StreamPuller) recoveryLoop(ctx context.Context) {
    ticker := time.NewTicker(30 * time.Second)
    defer ticker.Stop()

    for {
        select {
        case <-ctx.Done():
            return
        case <-p.done:
            return
        case <-ticker.C:
            // Skip processing if we don't have a handler
            if p.handler == nil {
                continue
            }

            // Check each partition for stale messages
            for partition := 0; partition < p.partitionCount; partition++ {
                streamKey := p.getStreamKey(partition)

                // Retrieve pending messages that have been idle longer than idleTime
                pendingMessages, err := p.redisClient.XPendingExt(
                    ctx,
                    streamKey,
                    p.subscription,
                    "-", // Start
                    "+", // End
                    10,  // Count
                    p.idleTime,
                )
                // ...bá» qua...

                if len(pendingMessages) == 0 {
                    continue
                }

                // Collect message IDs
                messageIDs := make([]string, len(pendingMessages))
                for i, pm := range pendingMessages {
                    messageIDs[i] = pm.ID
                }

                // Claim the messages for the current consumer
                claimed, err := p.redisClient.XClaim(
                    ctx,
                    streamKey,
                    p.subscription,
                    p.consumer,
                    p.idleTime,
                    messageIDs,
                )
                // ...bá» qua...

                // Reprocess the claimed messages
                p.reprocessClaimedMessages(ctx, claimed, streamKey)
            }
        }
    }
}
```

Äiá»ƒm cáº§n chÃº Ã½ á»Ÿ Ä‘Ã¢y lÃ  check interval vÃ  threshold lÃ  cÃ¡c cáº¥u hÃ¬nh riÃªng biá»‡t.

- **Check interval**: 30 giÃ¢y (`time.NewTicker(30 * time.Second)`)
- **Threshold**: 60 giÃ¢y (`defaultIdleTime = 60 * time.Second`)

Kiá»ƒm tra má»—i 30 giÃ¢y vÃ  thu há»“i cÃ¡c messages chÆ°a Ä‘Æ°á»£c xá»­ lÃ½ trong hÆ¡n 60 giÃ¢y. Thiáº¿t káº¿ nÃ y cho phÃ©p phÃ¢n biá»‡t giá»¯a Ä‘á»™ trá»… táº¡m thá»i vÃ  messages thá»±c sá»± bá»‹ tá»“n Ä‘á»ng.

---

## 2. Routing vá»›i Nginx

Bucketeer há»— trá»£ 3 phÆ°Æ¡ng thá»©c giao tiáº¿p sau:

| PhÆ°Æ¡ng thá»©c | Má»¥c Ä‘Ã­ch sá»­ dá»¥ng | Protocol |
|---------|------|-----------|
| gRPC | Giao tiáº¿p giá»¯a cÃ¡c services, SDK | HTTP/2 |
| gRPC-Web (dá»± kiáº¿n chuyá»ƒn sang REST) | API calls tá»« browser | HTTP/1.1 |
| REST | REST API (qua gRPC-Gateway) | HTTP/1.1 |

Trong mÃ´i trÆ°á»ng Minikube, Envoy xá»­ lÃ½ routing cho cÃ¡c phÆ°Æ¡ng thá»©c nÃ y, nhÆ°ng trong mÃ´i trÆ°á»ng Docker Compose, chÃºng tÃ´i cáº§n thá»±c hiá»‡n báº±ng Nginx. VÃ¬ mÃ´i trÆ°á»ng Docker Compose khÃ´ng cÃ³ cÆ¡ cháº¿ sidecar hoáº·c service discovery nhÆ° Kubernetes, chÃºng tÃ´i sá»­ dá»¥ng Nginx nhÆ° má»™t reverse proxy duy nháº¥t.

Váº¥n Ä‘á» á»Ÿ Ä‘Ã¢y lÃ  phÆ°Æ¡ng phÃ¡p xá»­ lÃ½ khÃ¡c nhau giá»¯a gRPC vÃ  gRPC-Web.

- **gRPC**: HTTP/2 proxy vá»›i directive `grpc_pass`
- **gRPC-Web**: HTTP/1.1 proxy vá»›i directive `proxy_pass`

Cáº§n phÃ¢n phá»‘i cÃ¡c requests Ä‘áº¿n cÃ¹ng má»™t URL path (vÃ­ dá»¥: `/bucketeer.account.AccountService`) dá»±a trÃªn Content-Type.

### PhÃ¡n ÄoÃ¡n PhÆ°Æ¡ng Thá»©c Giao Tiáº¿p Báº±ng map Directive

Sá»­ dá»¥ng `map` directive cá»§a Nginx Ä‘á»ƒ phÃ¡n Ä‘oÃ¡n phÆ°Æ¡ng thá»©c giao tiáº¿p tá»« Content-Type cá»§a request.

```nginx
# docker-compose/config/nginx/bucketeer.conf

# Map to detect gRPC-Web requests
map $http_content_type $is_grpc_web {
    ~*application/grpc-web 1;
    default 0;
}
```

Vá»›i cáº¥u hÃ¬nh nÃ y, cÃ¡c requests cÃ³ `Content-Type: application/grpc-web` sáº½ cÃ³ `$is_grpc_web = 1`, cÃ¡c requests khÃ¡c sáº½ cÃ³ `$is_grpc_web = 0`.

Káº¿t quáº£ phÃ¡n Ä‘oÃ¡n Ä‘Æ°á»£c lÆ°u trong biáº¿n nÃªn cÃ³ thá»ƒ tham chiáº¿u trong cÃ¡c location blocks tiáº¿p theo. PhÃ¡n Ä‘oÃ¡n chá»‰ Ä‘Æ°á»£c thá»±c hiá»‡n má»™t láº§n cho má»—i request nÃªn hiá»‡u quáº£.

### Internal Redirect vá»›i 418 Error Page

Directive `if` cá»§a Nginx cÃ³ nhiá»u háº¡n cháº¿ vÃ  khÃ´ng thá»ƒ chuyá»ƒn Ä‘á»•i `grpc_pass` hoáº·c `proxy_pass` bÃªn trong `if`.

Do Ä‘Ã³, chÃºng tÃ´i sá»­ dá»¥ng ká»¹ thuáº­t internal redirect vá»›i `error_page`.

```nginx
# docker-compose/config/nginx/bucketeer.conf

# gRPC/gRPC-Web service routes (backend handles both protocols)
location /bucketeer.account.AccountService {
    if ($is_grpc_web = 1) {
        error_page 418 = @grpc_web_account;
        return 418;
    }
    grpc_pass grpcs://web_account_backend;
    grpc_connect_timeout 15s;
    grpc_send_timeout 15s;
    grpc_read_timeout 15s;
}

location @grpc_web_account {
    proxy_pass https://web_account_backend;
}
```

Luá»“ng xá»­ lÃ½ nhÆ° sau:

1. Request Ä‘áº¿n `/bucketeer.account.AccountService`
2. Náº¿u `$is_grpc_web = 1`, redirect Ä‘áº¿n `@grpc_web_account` báº±ng `error_page 418`
3. Xá»­ lÃ½ táº¡i `@grpc_web_account` báº±ng `proxy_pass` (HTTP/1.1)
4. Náº¿u `$is_grpc_web = 0`, xá»­ lÃ½ báº±ng `grpc_pass` (HTTP/2)

418 lÃ  status code Ä‘Æ°á»£c biáº¿t Ä‘áº¿n nhÆ° "I'm a teapot". VÃ¬ khÃ´ng Ä‘Æ°á»£c sá»­ dá»¥ng trong xá»­ lÃ½ lá»—i thá»±c táº¿ nÃªn chÃºng tÃ´i táº­n dá»¥ng cho má»¥c Ä‘Ã­ch internal redirect.

Timeout Ä‘Æ°á»£c Ä‘iá»u chá»‰nh theo Ä‘áº·c tÃ­nh cá»§a service, API thÃ´ng thÆ°á»ng Ä‘Æ°á»£c Ä‘áº·t 15 giÃ¢y, batch processing Ä‘Æ°á»£c Ä‘áº·t 1 giá» (3600 giÃ¢y).

### Cáº¥u HÃ¬nh CORS

VÃ¬ gRPC-Web Ä‘Æ°á»£c gá»i tá»« browser, cáº§n cáº¥u hÃ¬nh CORS (Cross-Origin Resource Sharing).

```nginx
# docker-compose/config/nginx/bucketeer.conf

# CORS headers for gRPC-Web support
add_header 'Access-Control-Allow-Origin' '*' always;
add_header 'Access-Control-Allow-Methods' 'GET, POST, OPTIONS, PUT, DELETE, PATCH' always;
add_header 'Access-Control-Allow-Headers' 'Content-Type, x-grpc-web, authorization, grpc-timeout' always;
add_header 'Access-Control-Allow-Credentials' 'true' always;
add_header 'Access-Control-Expose-Headers' 'Content-Length, Content-Encoding, grpc-message, grpc-status, grpc-status-details-bin' always;

# Handle preflight OPTIONS requests
if ($request_method = 'OPTIONS') {
    return 204;
}
```

3 Ä‘iá»ƒm quan trá»ng:

1. **Cho phÃ©p header `x-grpc-web`**: Header Ä‘áº·c trÆ°ng cá»§a gRPC-Web
2. **Public hÃ³a gRPC status**: Tráº£ vá» `grpc-message`, `grpc-status`, v.v. cho browser
3. **Xá»­ lÃ½ preflight request**: Tráº£ vá» 204 cho OPTIONS request mÃ  browser gá»­i trÆ°á»›c

TLS Bridging vÃ  kÃ­ch hoáº¡t HTTP/2 cÅ©ng Ä‘Æ°á»£c thá»±c hiá»‡n bá»Ÿi Nginx, káº¿t ná»‘i mÃ£ hÃ³a Ä‘áº¿n backend báº±ng `grpcs://`.

---

## Tá»•ng Káº¿t

BÃ i viáº¿t nÃ y Ä‘Ã£ giá»›i thiá»‡u 2 thiáº¿t káº¿ Ä‘Æ°á»£c implement trong viá»‡c tÃ­ch há»£p Docker Compose cá»§a Bucketeer.

1. **Messaging vá»›i Redis Streams**
2. **Routing vá»›i Nginx**

LÆ°u Ã½ ráº±ng thiáº¿t káº¿ Ä‘Æ°á»£c giá»›i thiá»‡u trong bÃ i viáº¿t nÃ y nháº±m má»¥c Ä‘Ã­ch tÃ¡i táº¡o trong mÃ´i trÆ°á»ng development local. Náº¿u Ã¡p dá»¥ng cáº¥u hÃ¬nh tÆ°Æ¡ng tá»± trong mÃ´i trÆ°á»ng production, cáº§n xem xÃ©t thÃªm vá» giá»›i háº¡n throughput cá»§a Redis Streams, cáº¥u hÃ¬nh chi tiáº¿t cá»§a Nginx, v.v.

Náº¿u báº¡n cÃ³ Ã½ kiáº¿n "cÃ¡ch nÃ y tá»‘t hÆ¡n nhÃ©" hoáº·c "Ä‘iá»u nÃ y nghÄ©a lÃ  gÃ¬ nhá»‰", chÃºng tÃ´i ráº¥t mong nháº­n Ä‘Æ°á»£c issues vÃ  PRs! [Bucketeer GitHub Issues](https://github.com/bucketeer-io/bucketeer/issues)

Cáº£m Æ¡n báº¡n Ä‘Ã£ Ä‘á»c Ä‘áº¿n cuá»‘i!

---

## TÃ i Liá»‡u Tham Kháº£o

- [Bucketeer GitHub Repository](https://github.com/bucketeer-io/bucketeer)
- [Khá»Ÿi Ä‘áº§u OSS hÃ³a Bucketeer](https://www.cyberagent.co.jp/way/list/detail/id=28416)
- [CyberAgent Developers Advent Calendar 2025](https://adventar.org/calendars/11590)

---

**Tags**: #Bucketeer #Golang #kubernetes #Infrastructure #FeatureFlags
